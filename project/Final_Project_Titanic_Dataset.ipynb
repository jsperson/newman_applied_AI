{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abada375-1793-41cd-ba62-0b249d37f764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Project - Titanic Dataset \n",
    "**Jason \"Scott\" Person**\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "Fields include:\n",
    "\n",
    "- **Name** (str) - Name of the passenger\n",
    "- **Pclass** (int) - Ticket class\n",
    "- **Sex** (str) - Sex of the passenger\n",
    "- **Age** (float) - Age in years\n",
    "- **SibSp** (int) - Number of siblings and spouses aboard\n",
    "- **Parch** (int) - Number of parents and children aboard\n",
    "- **Ticket** (str) - Ticket number\n",
    "- **Fare** (float) - Ticket price paid\n",
    "- **Cabin** (str) - Cabin number\n",
    "- **Embarked** (str) - Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d2369c5-94d1-4199-b443-e06856987826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import libraries required to load, transform, analyze and plot data\n",
    "# this is from the churn analysis notebook, which is the foundation for this project solution\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(context='paper', style='darkgrid', \n",
    "        rc={'figure.facecolor':'white'}, font_scale=1.2)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, roc_auc_score\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14407e5f-5980-4b52-a871-f338dea3c19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Customize seaborn plot styles\n",
    "# Seaborn docs: https://seaborn.pydata.org/tutorial/aesthetics.html\n",
    "\n",
    "# Adjust to retina quality\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n",
    "\n",
    "# Adjust dpi and font size\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':300})\n",
    "sns.set_context('notebook', font_scale = 0.8)\n",
    "\n",
    "# Display tick marks\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Remove borders\n",
    "plt.rc('axes.spines', top=False, right=False, left=False, bottom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0fe688e-1669-499b-a294-30291e0d1f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Color palettes for plots\n",
    "# Named colors: https://matplotlib.org/stable/gallery/color/named_colors.html\n",
    "# Seaborn color palette docs: https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "# Seaborn palette chart: https://www.codecademy.com/article/seaborn-design-ii\n",
    "\n",
    "# cp1 Color Palette - a binary blue/orange palette\n",
    "blue = 'deepskyblue' # Use 'skyblue' for a lighter blue\n",
    "orange = 'orange'\n",
    "cp1 = [blue, orange]\n",
    "\n",
    "# cp2 Color Palette - 5 colors for use with categorical data\n",
    "turquoise = 'mediumaquamarine'\n",
    "salmon = 'darksalmon'\n",
    "tan = 'tan'\n",
    "gray = 'darkgray'\n",
    "cp2 = [blue, turquoise, salmon, tan, gray]\n",
    "\n",
    "# cp3 Color Palette - blue-to-orange diverging palette for correlation heatmaps\n",
    "cp3 = sns.diverging_palette(242, 39, s=100, l=65, n=11)\n",
    "\n",
    "# Set the default palette\n",
    "sns.set_palette(cp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69f2709-b189-404a-b85b-9bb0447d090b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c15173-c44e-4bff-ba08-e31a1ff4309e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View dataframe fundamentals\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe3e9be0-8979-430e-8b62-5b034143f298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Explore Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6241cf4-ff15-44e3-b21b-a8fcb379df4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check value counts by column\n",
    "col_list = ['Pclass', 'Sex', 'Fare', 'Embarked']\n",
    "\n",
    "for col in col_list:\n",
    "     print(f'\\nValue Counts | column = {col}')\n",
    "     print(df[col].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f195a9ac-dca1-4451-a45b-22749c839708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Calculate required data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e170d0f-ff5f-4e1b-811c-73199324a229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Median age by Sex and Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fbf178c-6b4b-4634-9bc3-3e94f554356e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_median_age(X_df):\n",
    "    \"\"\"Calculates the median age for each Sex and Pclass group.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    median_ages_df (pd.DataFrame)): dataframe with median ages for each Sex and Pclass group\n",
    "    \"\"\"\n",
    "    median_ages = X_df.groupby(['Sex', 'Pclass'])['Age'].median().reset_index()\n",
    "    median_ages_df = median_ages.rename(columns={'Age': 'Median_Age'})\n",
    "    return median_ages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46b7cd7-5401-4806-93de-ef9ebe1953a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_and_persist_median_ages(X_df):\n",
    "    \"\"\"Calculates and persists the median age dataframe to storage.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    median_ages_df = calculate_median_age(X_df)\n",
    "    median_ages_df.to_csv('median_ages.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871d2f9f-3ed0-4a99-a796-7d5a619bf82e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Median third class fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc76d9f8-faec-4bea-bb19-7ab6b3948247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_class_median_fare(X_df):\n",
    "    \"\"\"Fill missing fare values in the Fare field. There is one missing value and it is a third class passenger so we're going to use the median fare for that class.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    median_fare_df (pd.DataFrame): dataframe with the median fare for third class passengers\n",
    "    \"\"\"\n",
    "\n",
    "    med_fare = X_df.groupby(['Pclass']).Fare.median().reset_index()\n",
    "    median_fare_df = med_fare.rename(columns={'Fare': 'Median_Fare'})\n",
    "\n",
    "    return median_fare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd918825-fefa-4f51-8a91-b0eb616d200f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_and_persist_fare(X_df):\n",
    "    \"\"\"Calls calculate_class_median_fare function to fill missing fare values and persists the median fare to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame): Dataframe with filled fare values\n",
    "    \"\"\"\n",
    "    median_class_fares = calculate_class_median_fare(X_df)\n",
    "    median_class_fares.to_csv('median_class_fares.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e68e15-6519-4357-94c3-032b3b1f21c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Splitter persister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709fd0be-23ce-41dd-bcf4-e5bafeb4d23c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bins(X_df, column, bin_count):\n",
    "    \"\"\"Calculates the bins for the specified column using qcut and returns the splits in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): Dataframe containing the data\n",
    "    column (str): The column to calculate bins for\n",
    "    bins (int): The number of bins to split the data into\n",
    "\n",
    "    Returns:\n",
    "    bins_df (pd.DataFrame): Dataframe with the bin edges\n",
    "    \"\"\"\n",
    "    bin_edges = pd.qcut(X_df[column], q=bin_count, retbins=True)[1]\n",
    "    bins_df = pd.DataFrame({'Bin_Edges': bin_edges})\n",
    "    \n",
    "    return bins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860620c6-c65e-4805-8a2a-aed65e0ca158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_and_persist_fare_bins(X_df):\n",
    "    \"\"\"Calls calculate_bins on the Fare column with x bins and persists the returned dataframe to fare_splist.csv.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "    bins_df (pd.DataFrame): Dataframe with the bin edges\n",
    "    \"\"\"\n",
    "    bin_count = 5\n",
    "    bins_df = calculate_bins(X_df, 'Fare', bin_count)\n",
    "    bins_df.to_csv('fare_splits.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1a197a-1ec5-4a5e-b004-e1514f58c1f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006b5600-997c-4285-9701-7f212c8fcd02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1c5458-13c6-448b-9a51-5c1fc9160336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### fill_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd12e440-c70b-4edf-9cdb-c3828d3671f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fill age with median from group\n",
    "def fill_age(X_df):\n",
    "    \"\"\"Fills missing age values in the age field using the provided age dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with replaced values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load ages from storage - this would go in production pipeline\n",
    "    median_ages_df = pd.read_csv('median_ages.csv')\n",
    "\n",
    "    # Create a dictionary for median ages from age_df\n",
    "    median_ages = median_ages_df.set_index(['Sex', 'Pclass'])['Median_Age'].to_dict()\n",
    "    \n",
    "    # Setting Age to the median value based on Sex and Pclass only when Age is not a number\n",
    "    X_df['age'] = X_df.apply(lambda row: median_ages.get((row['sex'], row['pclass'])) if pd.isnull(row['age']) else row['age'], axis=1)\n",
    "    \n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803b46a3-f1f0-422e-b54c-718319dda5ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### fill_embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6954251-1e8b-449c-986a-7058f9cd6282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fill embarked with 'S'\n",
    "\n",
    "def fill_embarked(X_df):\n",
    "    \"\"\"Fill missing embarded values in the Embarked field. We use S based on analysis by Evitan that showed that these two passengers actually embarked at Southampton.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with replaced values\n",
    "    \"\"\"\n",
    "\n",
    "    # Filling the missing values in Embarked with S\n",
    "    X_df['embarked'] = X_df['embarked'].fillna('S')\n",
    "    \n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a178dd-455e-4c93-902b-101a08786918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### fill_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7292bf5b-320c-463e-b585-39b552b0d96b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fill_fare(X_df):\n",
    "    \"\"\"Fill missing fare values in the Fare field using the median fare for the same class.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with replaced values\n",
    "    \"\"\"\n",
    "\n",
    "    # Load median fares from storage - this would go in production pipeline\n",
    "    median_class_fares_df = pd.read_csv('median_class_fares.csv')\n",
    "\n",
    "    # Create a dictionary for median fares from median_class_fares_df\n",
    "    median_class_fares = median_class_fares_df.set_index('Pclass')['Median_Fare'].to_dict()\n",
    "    \n",
    "    # Filling the missing value in Fare with the median fare for the same class\n",
    "    X_df['fare'] = X_df.apply(lambda row: median_class_fares.get(row['pclass']) if pd.isna(row['fare']) else row['fare'], axis=1)\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f51d1d0-7b38-4a1e-9517-d6607473267a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### bin_fare_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac592089-f459-4ff0-b1d6-59b8f8ca7417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bin_age(X_df):\n",
    "    \"\"\"Creates calculated fields Family_count and groups it then bins fare and age\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with new columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Define age edges\n",
    "    age_edges = [0, 12, 17, 30, 50, 100]\n",
    "    age_labels = ['Child', 'Teen', 'YoungAdult', 'Adult', 'Senior']\n",
    "\n",
    "    # Bin Age using predefined edges\n",
    "    X_df['age_bin'] = pd.cut(X_df['age'], bins=age_edges, labels=age_labels)\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52c9366-e4ae-4b0c-abf7-56eac78adddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bin_fare(X_df):\n",
    "    \"\"\"Bins fare using predefined edges from fare_edges.csv.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with new columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Load fare edges from fare_edges.csv\n",
    "    fare_edges_df = pd.read_csv('fare_splits.csv')\n",
    "    fare_edges = fare_edges_df['Bin_Edges'].tolist()\n",
    "\n",
    "    # Bin fare using predefined edges\n",
    "    X_df['fare_bin'] = pd.cut(X_df['fare'], bins=fare_edges)\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d73703-8d51-4675-9dd6-c54521c75d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### bin_family_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc955ea1-4737-4f9d-8375-94d220744685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bin_family_count(X_df):\n",
    "    \"\"\"Creates calculated fields Family_count and groups it\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with new columns\n",
    "    \"\"\"\n",
    "    # Family count\n",
    "    X_df['family_count'] = X_df['sibsp'] + X_df['parch']\n",
    "\n",
    "    # Bin family size\n",
    "    family_map = {0: 'Alone', 1: 'Small', 2: 'Small', 3: 'Small', 4: 'Medium', 5: 'Medium', 6: 'Large', 7: 'Large', 10: 'Large'}\n",
    "    X_df['family_count_bin'] = X_df['family_count'].map(family_map)\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c584a201-5623-4de9-a362-02c61c9507ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### create_title_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc700fb9-4a16-4383-8137-02b5e424cf1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_title_feature(X_df):\n",
    "    \"\"\"Creates title feature and groups it; interleave the Is_married feature as well\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with new columns\n",
    "    \"\"\"    \n",
    "\n",
    "    X_df['title'] = X_df['name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
    "\n",
    "    X_df['is_married'] = 0\n",
    "    X_df['is_married'].loc[X_df['title'] == 'Mrs'] = 1\n",
    "\n",
    "    X_df['title'] = X_df['title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
    "    X_df['title'] = X_df['title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8bc1cc-8712-42b4-9c92-8403041aa845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### create_deck_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c83f24-c959-44c6-809b-0d75a9cbbbaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_deck_feature(X_df):\n",
    "    \"\"\"Creates deck feature\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with new columns\n",
    "    \"\"\"    \n",
    "\n",
    "    X_df['deck'] = X_df['cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71ab017-b10f-446e-82c1-68446aa336bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### one_hot_encode_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6299721-8d49-446a-a79d-8facafe166d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert categorical columns to one-hot encoding features\n",
    "def ohe_categories(X_df):\n",
    "    \"\"\"Creates one-hot encoded (OHE) features for a list of categorical columns \n",
    "    and simplifies column names.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with OHE columns\n",
    "    \"\"\"\n",
    "\n",
    "    # create list of multi-class variables for one-hot encoding\n",
    "    categoricals = ['pclass', 'sex', 'embarked', 'title', 'deck', 'family_count_bin', 'fare_bin','age_bin']\n",
    "\n",
    "    # Without this line, I was just getting another Pclass column. Took about an hour to figure out. This feels really kludgy.\n",
    "    X_df['pclass'] = X_df['pclass'].astype(str)\n",
    "\n",
    "    # create one-hot encoded dummy variables for categoricals\n",
    "    # leaving this in so that I have an example in the future\n",
    "    X_df_ohe = pd.get_dummies(X_df[categoricals], drop_first=False, dtype=int)\n",
    "    X_df_ohe.rename(\n",
    "        columns={'sex_male' : 'sex_male',\n",
    "                 'sex_female' : 'sex_female'\n",
    "                }, inplace = True)\n",
    "    \n",
    "    # concatenate OHE with original df, and drop original category columns\n",
    "    X_df = pd.concat([X_df, X_df_ohe], axis=1)\n",
    "    X_df.drop(categoricals, axis=1, inplace=True)\n",
    "    \n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a8e616-ca00-4c97-8ac4-0a4d4a428be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### lower_case_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5824fd76-2c2b-49fb-b6da-f6c3cdb61593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_columns_lowercase(X_df):\n",
    "    \"\"\"Renames all column names to lowercase in place.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): DataFrame whose columns need to be renamed\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    X_df.columns = [col.lower() for col in X_df.columns]\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76610694-5d60-4e01-984c-23946bb0553d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop Unneeded Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d05ee0e-0c90-4273-9ed0-4b52ad75ad8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_columns(X_df, columns_to_drop):\n",
    "    \"\"\"Drops specified columns from the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): DataFrame from which columns need to be dropped\n",
    "    columns_to_drop (list): List of column names to drop\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame): DataFrame with specified columns dropped\n",
    "    \"\"\"\n",
    "    X_df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7cae4c-2849-4885-be87-9a93967bf5ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_specified_columns(X_df):\n",
    "    \"\"\"Drops specified columns from the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): DataFrame from which columns need to be dropped\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame): DataFrame with specified columns dropped\n",
    "    \"\"\"\n",
    "    columns_to_drop = ['passengerid', 'name', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'family_count', 'is_married']\n",
    "    return drop_columns(X_df, columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89e56cd-7c04-4fd9-8691-13e67fbc6f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Prep Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c225b333-556d-4108-a4f4-2dd2dc50f8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function holds data preparation pipeline for X predictors dataframe\n",
    "def data_prep_pipe(X_df):\n",
    "    \"\"\"Executes data preparation pipeline of steps to clean and transform\n",
    "    an X features dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df_tr (pd.DataFrame)): train or test dataframe, transformed\n",
    "    \"\"\"\n",
    "    \n",
    "    # instantiate custom transformer functions\n",
    "    get_fill_age = FunctionTransformer(fill_age, validate=False)\n",
    "    get_fill_embarked = FunctionTransformer(fill_embarked, validate=False)\n",
    "    get_fill_fare = FunctionTransformer(fill_fare, validate=False)\n",
    "    get_bin_age = FunctionTransformer(bin_age, validate=False)\n",
    "    get_bin_fare = FunctionTransformer(bin_fare, validate=False)\n",
    "    get_bin_family_count = FunctionTransformer(bin_family_count, validate=False)\n",
    "    get_create_title_feature = FunctionTransformer(create_title_feature, validate=False)\n",
    "    get_create_deck_feature = FunctionTransformer(create_deck_feature, validate=False)\n",
    "    get_ohe_categories = FunctionTransformer(ohe_categories, validate=False)\n",
    "    get_rename_columns_lowercase = FunctionTransformer(rename_columns_lowercase, validate=False)\n",
    "    get_drop_specified_columns = FunctionTransformer(drop_specified_columns, validate=False)\n",
    "\n",
    "    # instantiate data prep pipeline object and steps\n",
    "    prep_pipe = Pipeline(memory=None, \n",
    "                         steps=[('rename_columns_lowercase', get_rename_columns_lowercase),\n",
    "                                ('fill_age', get_fill_age),\n",
    "                                ('fill_embarked', get_fill_embarked),\n",
    "                                ('fill_fare', get_fill_fare),\n",
    "                                ('bin_age', get_bin_age),\n",
    "                                ('bin_fare', get_bin_fare),\n",
    "                                ('bin_family_count', get_bin_family_count),\n",
    "                                ('create_title_feature', get_create_title_feature),\n",
    "                                ('create_deck_feature', get_create_deck_feature),\n",
    "                                ('ohe_categories', get_ohe_categories),\n",
    "                                ('drop_specified_columns', get_drop_specified_columns),\n",
    "                                ('rename_columns_lowercase_again', get_rename_columns_lowercase)\n",
    "                                ])\n",
    "    \n",
    "    # apply data prep pipeline to df and store/return new df\n",
    "    X_df_tr = prep_pipe.fit_transform(X_df)\n",
    "    return X_df_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4a4b45-52fd-47fe-a54d-ab564016c299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894534ea-bca3-41d4-b901-cce86a93bec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d87f79-461a-4193-82aa-58ca4e60e43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create X predictors and y target variable\n",
    "y = df['Survived']\n",
    "X = df.drop(columns=['Survived'], axis=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "SEED = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af07ca8f-1e9e-4bd7-ac4b-78c92b725b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Calculate Persisted Data Prep Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182d2351-bed0-4487-97f1-a6535a5993ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate and persist the median ages and fares to fill in the missing data. This is done against the training set only to prevent leakage.\n",
    "# Eventually I would refactor this to use the pipeline - just don't have time right now!\n",
    "# Note: Evitan used the union of training and test data to calculate averages. I believe that this may be an error. https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial block In [7]\n",
    "\n",
    "process_and_persist_median_ages(X_train)\n",
    "process_and_persist_fare(X_train)\n",
    "calculate_and_persist_fare_bins(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbfa2fd3-6a98-418d-8ff0-8d811f8b0c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2783b784-e862-4cec-abbc-1dd158cc9bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# send both X_train and X_test through data prep steps\n",
    "X_train = data_prep_pipe(X_train)\n",
    "X_test = data_prep_pipe(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f558b66d-41e6-45f8-b0a8-d566b49dd088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be1ee5b-5666-4485-8cb7-28083548353b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2ed44a-6830-4f94-91e4-c229ff1b3d97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d8a561-1f6e-4087-a066-87eaad0fe6f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bc9b44e-766a-4f27-b46a-9f140eaa17bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e862b7ea-bf07-4c42-81eb-9eed556fe41d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f69247-31cc-405c-b0fe-0b26d32d7771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca36bbf-b948-49e3-8973-32f1b7c628c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab6605d-ac77-462c-be3e-6d64986ed2df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(df['Pclass'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1151c9-c9cd-4108-84f9-8de6e0ee2380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final_Project_Titanic_Dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
