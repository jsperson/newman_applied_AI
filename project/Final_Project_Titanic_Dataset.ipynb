{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abada375-1793-41cd-ba62-0b249d37f764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Project - Titanic Dataset \n",
    "**Jason \"Scott\" Person**\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "Fields include:\n",
    "\n",
    "- **Name** (str) - Name of the passenger\n",
    "- **Pclass** (int) - Ticket class\n",
    "- **Sex** (str) - Sex of the passenger\n",
    "- **Age** (float) - Age in years\n",
    "- **SibSp** (int) - Number of siblings and spouses aboard\n",
    "- **Parch** (int) - Number of parents and children aboard\n",
    "- **Ticket** (str) - Ticket number\n",
    "- **Fare** (float) - Ticket price paid\n",
    "- **Cabin** (str) - Cabin number\n",
    "- **Embarked** (str) - Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ce588e-0d3e-41d6-a69b-4704cbfa376e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Todo Items\n",
    "- Figure out how to get best random out of random forest\n",
    "- Investigate weights for each feature parameter\n",
    "- SMOTE (optional)\n",
    "- Sex - should it be labeled or OHE?\n",
    "- Bin Deck\n",
    "- Pclass string kludge\n",
    "- Serialize data preprocessing\n",
    "- Serialize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1344dabf-859c-443a-85d9-69a71d16eec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Talking Points\n",
    "### Goals:\n",
    "1. Learn \n",
    "1. Meet Requirements\n",
    "1. Consider production implications\n",
    "\n",
    "### Lessons Learned:\n",
    "1. Model training is not deterministic\n",
    "1. There's a lot of art here - it's not just data science!\n",
    "1. In the weeds:<br>\n",
    "  3.1. \n",
    "  Calculate bins and imputed values on training set only <br>\n",
    "  3.2. Training and test columns need to align (OHE issue)<br>\n",
    "  3.3. OHE doesn't like integers <br>\n",
    "  3.4. Label encoding works better than OHE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3421b526-69b0-4850-924e-2fbf0d6c2130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3d2369c5-94d1-4199-b443-e06856987826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import libraries required to load, transform, analyze and plot data\n",
    "# this is from the churn analysis notebook, which is the foundation for this project solution\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "%matplotlib inline\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set(context='paper', style='darkgrid', \n",
    "        rc={'figure.facecolor':'white'}, font_scale=1.2)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, roc_auc_score\n",
    "# from sklearn import tree\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "14407e5f-5980-4b52-a871-f338dea3c19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Customize seaborn plot styles\n",
    "# Seaborn docs: https://seaborn.pydata.org/tutorial/aesthetics.html\n",
    "\n",
    "# Adjust to retina quality\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"retina\")\n",
    "\n",
    "# Adjust dpi and font size\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':300})\n",
    "sns.set_context('notebook', font_scale = 0.8)\n",
    "\n",
    "# Display tick marks\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Remove borders\n",
    "plt.rc('axes.spines', top=False, right=False, left=False, bottom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d0fe688e-1669-499b-a294-30291e0d1f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Color palettes for plots\n",
    "# Named colors: https://matplotlib.org/stable/gallery/color/named_colors.html\n",
    "# Seaborn color palette docs: https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "# Seaborn palette chart: https://www.codecademy.com/article/seaborn-design-ii\n",
    "\n",
    "# cp1 Color Palette - a binary blue/orange palette\n",
    "blue = 'deepskyblue' # Use 'skyblue' for a lighter blue\n",
    "orange = 'orange'\n",
    "cp1 = [blue, orange]\n",
    "\n",
    "# cp2 Color Palette - 5 colors for use with categorical data\n",
    "turquoise = 'mediumaquamarine'\n",
    "salmon = 'darksalmon'\n",
    "tan = 'tan'\n",
    "gray = 'darkgray'\n",
    "cp2 = [blue, turquoise, salmon, tan, gray]\n",
    "\n",
    "# cp3 Color Palette - blue-to-orange diverging palette for correlation heatmaps\n",
    "cp3 = sns.diverging_palette(242, 39, s=100, l=65, n=11)\n",
    "\n",
    "# Set the default palette\n",
    "sns.set_palette(cp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b69f2709-b189-404a-b85b-9bb0447d090b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "64c15173-c44e-4bff-ba08-e31a1ff4309e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View dataframe fundamentals\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe3e9be0-8979-430e-8b62-5b034143f298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Explore Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6241cf4-ff15-44e3-b21b-a8fcb379df4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check value counts by column\n",
    "col_list = ['Pclass', 'Sex', 'Fare', 'Embarked']\n",
    "\n",
    "for col in col_list:\n",
    "     print(f'\\nValue Counts | column = {col}')\n",
    "     print(df[col].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70fdab2-c831-49c2-9996-2ec2ec71003a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Discuss Modeling and Data Prep Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df601d44-0d69-4b96-be70-a5fa809ba3e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze Age for Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ce3bb9-1752-4eb4-b713-165781cbcd27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Histogram: Age Distribution Comparisons by Survival\n",
    "plt.title(\"Age Distributions Comparison\", fontsize=14, fontweight='bold')\n",
    "ax = sns.histplot(data=df, x='Age', hue='Survived', binwidth=5, alpha=0.7);\n",
    "# ax.set(xlabel = 'Custom x axis label', ylabel='Custom y axis label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af21f449-9c21-4921-b5b4-d0d09bd29c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Interpretation:** We took a look at this during our first module. The interesting characteristic is the outsized survival rate of the younger passengers. To capture this as well as to create bins that correspond to social norms I chose the following bins and respective labels:\n",
    "\n",
    "`age_edges = [0, 12, 18, 30, 50, 100]`  \n",
    "`age_labels = ['Child', 'Teen', 'YoungAdult', 'Adult', 'Senior']`\n",
    "\n",
    "During testing I did try a bin for infants in order to try to capture the even higher survival rate there, but it resulted in lower accuracy overall. My current hypothesis is that this led to overfitting of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b02d87-71f2-4622-9863-2b0317eb5784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze Fare for Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82be62e1-4ed9-4404-b34b-7e8b148864c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Histogram: Fare Distribution Comparisons by Survival\n",
    "plt.title(\"Fare Distributions Comparison\", fontsize=14, fontweight='bold')\n",
    "ax = sns.histplot(data=df, x='Fare', hue='Survived', binwidth=25, alpha=0.7);\n",
    "# ax.set(xlabel = 'Custom x axis label', ylabel='Custom y axis label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bafd5f1-252a-44b5-9fde-507e527d789c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Interpretation:** I spent a lot of time fiddling with fares. I wanted to achieve a couple of things:\n",
    "1. Use cut or qcut to figure out the bins rather than hard coding them.\n",
    "2. Persist the bins created with the training set only.\n",
    "\n",
    "Fromt he chart we can see that this is a widely dispersed dataset with outliers on the high end. pandas.qcut divides the data into approximatgely balanced quintiles so that should handle outliers better. I started with 5 bins, which gave me an 82.x% accuracy with logistic regression using default parameters. When I switched to 15 bins I got better results (83.x%) with Random Forrest, but only with 5-fold cross validation.\n",
    "\n",
    "I'll stick with 15 because that gives me a better result, but my takeaway here is that I need to do more research on the various algorithms.\n",
    "\n",
    "**Coding Note:** I originally just left the bins with default names. This resulted in dataframe columns of names like fare_[xx.x-yy.y]. Some of the modeling algorithms did not like brackets and parenthesis in the column nnames. I had to rename to form fare_n where n is just an incrementing integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a52e28-fa24-4cb7-8a60-c316bd015b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyze Family Count and Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a5ec76-797d-4588-8b6f-9236da41becf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Histogram: Family Count Distribution Comparisons by Survival\n",
    "plt.title(\"Family Count Distributions Comparison\", fontsize=14, fontweight='bold')\n",
    "ax = sns.histplot(data=df, x=df['SibSp'] + df['Parch'], hue='Survived', bins=10, alpha=0.7)\n",
    "ax.set(xlabel='Family Count')\n",
    "# ax.set(xlabel = 'Custom x axis label', ylabel='Custom y axis label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4104a01-9437-4cf5-88c4-aa3255f1d92f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Interpretation:** There are three notable breaks in the data. \n",
    "- Alone: About average survival rate. \n",
    "- 1-3 other family members: High chance of survival\n",
    "- 4-6: Small chance of survival\n",
    "- Greater than 6: Much lower chance of survival.\n",
    "\n",
    "`family_map = {0: 'Alone', 1: 'Small', 2: 'Small', 3: 'Small', 4: 'Medium', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 9: 'Large', 10: 'Large'}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16174204-a7eb-46c7-8fd9-82dd18dc2031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Title Feature and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d22b45b4-a3f1-4aaa-ad5c-6a927856c429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Histogram: Family Count Distribution Comparisons by Survival\n",
    "plt.title(\"Title Distributions Comparison\", fontsize=14, fontweight='bold')\n",
    "ax = sns.histplot(data=df, x=df['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0], hue='Survived', bins=10, alpha=0.7)\n",
    "ax.set(xlabel='Title Count')\n",
    "# ax.set(xlabel = 'Custom x axis label', ylabel='Custom y axis label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba64140-cf90-4694-89fc-623bebe4bf4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Interpretation:** I did not do a plot this before making some assumptions and doing a lot of experimentation. I tried \"military\" and \"nobility groups\", male/female combinations, and eventually settled on the below. This lowers the number of categories as instructed and works the best based on my analysis.\n",
    "\n",
    "`title_mapping = {\n",
    "'Mr': 'Mr',\n",
    "'Miss': 'Miss/Mrs/Ms',\n",
    "'Mrs': 'Miss/Mrs/Ms',\n",
    "'Master': 'Master',\n",
    "'Dr': 'Special',\n",
    "'Rev': 'Special',\n",
    "'Col': 'Special',\n",
    "'Major': 'Special',\n",
    "'Capt': 'Special',\n",
    "'Sir': 'Special',\n",
    "'Don': 'Special',\n",
    "'Lady': 'Special',\n",
    "'the Countess': 'Miss/Mrs/Ms',\n",
    "'Jonkheer': 'Special',\n",
    "'Mlle': 'Miss/Mrs/Ms',\n",
    "'Ms': 'Miss/Mrs/Ms',\n",
    "'Mme': 'Miss/Mrs/Ms'\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f195a9ac-dca1-4451-a45b-22749c839708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Calculate Required Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e170d0f-ff5f-4e1b-811c-73199324a229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Median age by Sex and Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fbf178c-6b4b-4634-9bc3-3e94f554356e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_median_age(X_df):\n",
    "    \"\"\"Calculates the median age for each Sex and Pclass group.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    median_ages_df (pd.DataFrame)): dataframe with median ages for each Sex and Pclass group\n",
    "    \"\"\"\n",
    "    median_ages = X_df.groupby(['Sex', 'Pclass'])['Age'].median().reset_index()\n",
    "    median_ages_df = median_ages.rename(columns={'Age': 'Median_Age'})\n",
    "    return median_ages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46b7cd7-5401-4806-93de-ef9ebe1953a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_and_persist_median_ages(X_df):\n",
    "    \"\"\"Calculates and persists the median age dataframe to storage.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    median_ages_df = calculate_median_age(X_df)\n",
    "    median_ages_df.to_csv('median_ages.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871d2f9f-3ed0-4a99-a796-7d5a619bf82e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Median third class fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc76d9f8-faec-4bea-bb19-7ab6b3948247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_class_median_fare(X_df):\n",
    "    \"\"\"Fill missing fare values in the Fare field. There is one missing value and it is a third class passenger so we're going to use the median fare for that class.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    median_fare_df (pd.DataFrame): dataframe with the median fare for third class passengers\n",
    "    \"\"\"\n",
    "\n",
    "    med_fare = X_df.groupby(['Pclass']).Fare.median().reset_index()\n",
    "    median_fare_df = med_fare.rename(columns={'Fare': 'Median_Fare'})\n",
    "\n",
    "    return median_fare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd918825-fefa-4f51-8a91-b0eb616d200f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_and_persist_fare(X_df):\n",
    "    \"\"\"Calls calculate_class_median_fare function to fill missing fare values and persists the median fare to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame): Dataframe with filled fare values\n",
    "    \"\"\"\n",
    "    median_class_fares = calculate_class_median_fare(X_df)\n",
    "    median_class_fares.to_csv('median_class_fares.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e68e15-6519-4357-94c3-032b3b1f21c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Splitter persister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "709fd0be-23ce-41dd-bcf4-e5bafeb4d23c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bins(X_df, column, bin_count):\n",
    "    \"\"\"Calculates the bins for the specified column using qcut and returns the splits in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): Dataframe containing the data\n",
    "    column (str): The column to calculate bins for\n",
    "    bins (int): The number of bins to split the data into\n",
    "\n",
    "    Returns:\n",
    "    bins_df (pd.DataFrame): Dataframe with the bin edges\n",
    "    \"\"\"\n",
    "    bin_edges = pd.qcut(X_df[column], q=bin_count, retbins=True)[1]\n",
    "    bins_df = pd.DataFrame({'Bin_Edges': bin_edges})\n",
    "    \n",
    "    return bins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860620c6-c65e-4805-8a2a-aed65e0ca158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_and_persist_fare_bins(X_df):\n",
    "    \"\"\"Calls calculate_bins on the Fare column with x bins and persists the returned dataframe to fare_splist.csv.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): Dataframe containing the data\n",
    "\n",
    "    Returns:\n",
    "    bins_df (pd.DataFrame): Dataframe with the bin edges\n",
    "    \"\"\"\n",
    "    bin_count = 15\n",
    "    bins_df = calculate_bins(X_df, 'Fare', bin_count)\n",
    "    bins_df.to_csv('fare_splits.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1a197a-1ec5-4a5e-b004-e1514f58c1f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Pipeline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006b5600-997c-4285-9701-7f212c8fcd02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1c5458-13c6-448b-9a51-5c1fc9160336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### fill_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd12e440-c70b-4edf-9cdb-c3828d3671f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fill age with median from group\n",
    "def fill_age(X_df):\n",
    "    \"\"\"Fills missing age values in the age field using the provided age dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with replaced values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load ages from storage - this would go in production pipeline\n",
    "    median_ages_df = pd.read_csv('median_ages.csv')\n",
    "\n",
    "    # Create a dictionary for median ages from age_df\n",
    "    median_ages = median_ages_df.set_index(['Sex', 'Pclass'])['Median_Age'].to_dict()\n",
    "    \n",
    "    # Setting Age to the median value based on Sex and Pclass only when Age is not a number\n",
    "    X_df['age'] = X_df.apply(lambda row: median_ages.get((row['sex'], row['pclass'])) if pd.isnull(row['age']) else row['age'], axis=1)\n",
    "    \n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803b46a3-f1f0-422e-b54c-718319dda5ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### fill_embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6954251-1e8b-449c-986a-7058f9cd6282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fill embarked with 'S'\n",
    "\n",
    "def fill_embarked(X_df):\n",
    "    \"\"\"Fill missing embarded values in the Embarked field. We use S based on analysis by Evitan that showed that these two passengers actually embarked at Southampton.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with replaced values\n",
    "    \"\"\"\n",
    "\n",
    "    # Filling the missing values in Embarked with S\n",
    "    X_df['embarked'] = X_df['embarked'].fillna('S')\n",
    "    \n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a178dd-455e-4c93-902b-101a08786918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### fill_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7292bf5b-320c-463e-b585-39b552b0d96b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fill_fare(X_df):\n",
    "    \"\"\"Fill missing fare values in the Fare field using the median fare for the same class.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with replaced values\n",
    "    \"\"\"\n",
    "\n",
    "    # Load median fares from storage - this would go in production pipeline\n",
    "    median_class_fares_df = pd.read_csv('median_class_fares.csv')\n",
    "\n",
    "    # Create a dictionary for median fares from median_class_fares_df\n",
    "    median_class_fares = median_class_fares_df.set_index('Pclass')['Median_Fare'].to_dict()\n",
    "    \n",
    "    # Filling the missing value in Fare with the median fare for the same class\n",
    "    X_df['fare'] = X_df.apply(lambda row: median_class_fares.get(row['pclass']) if pd.isna(row['fare']) else row['fare'], axis=1)\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f51d1d0-7b38-4a1e-9517-d6607473267a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### bin_fare_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac592089-f459-4ff0-b1d6-59b8f8ca7417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bin_age(X_df):\n",
    "    \"\"\"Creates calculated fields Family_count and groups it then bins fare and age\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with new columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Define age edges\n",
    "    age_edges = [0, 12, 18, 30, 50, 100]\n",
    "    age_labels = ['Child', 'Teen', 'YoungAdult', 'Adult', 'Senior']\n",
    "\n",
    "    # Bin Age using predefined edges\n",
    "    X_df['age_bin'] = pd.cut(X_df['age'], bins=age_edges, labels=age_labels)\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52c9366-e4ae-4b0c-abf7-56eac78adddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bin_fare(X_df):\n",
    "    \"\"\"Bins fare using predefined edges from fare_edges.csv.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with new columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Load fare edges from fare_edges.csv\n",
    "    fare_edges_df = pd.read_csv('fare_splits.csv')\n",
    "    fare_edges = fare_edges_df['Bin_Edges'].tolist()\n",
    "\n",
    "    # Bin fare using predefined edges\n",
    "    X_df['fare_bin'] = pd.cut(X_df['fare'], bins=fare_edges)\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4d73703-8d51-4675-9dd6-c54521c75d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### bin_family_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc955ea1-4737-4f9d-8375-94d220744685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bin_family_count(X_df):\n",
    "    \"\"\"Creates calculated fields Family_count and groups it\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with new columns\n",
    "    \"\"\"\n",
    "    # Family count\n",
    "    X_df['family_count'] = X_df['sibsp'] + X_df['parch']\n",
    "\n",
    "    # Bin family size\n",
    "    family_map = {0: 'Alone', 1: 'Small', 2: 'Small', 3: 'Small', 4: 'Medium', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 9: 'Large', 10: 'Large'}\n",
    "    X_df['family_count_bin'] = X_df['family_count'].map(family_map)\n",
    "\n",
    "    # Add \"is_alone\" feature - this made random forest slightly worse\n",
    "    # X_df['is_alone'] = X_df['family_count'].map(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c584a201-5623-4de9-a362-02c61c9507ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### create_title_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc700fb9-4a16-4383-8137-02b5e424cf1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_title_feature(X_df):\n",
    "    \"\"\"Creates title feature and groups it; interleave the Is_married feature as well\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with new columns\n",
    "    \"\"\"    \n",
    "\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr',\n",
    "        'Miss': 'Miss/Mrs/Ms',\n",
    "        'Mrs': 'Miss/Mrs/Ms',\n",
    "        'Master': 'Master',\n",
    "        'Dr': 'Special',\n",
    "        'Rev': 'Special',\n",
    "        'Col': 'Special',\n",
    "        'Major': 'Special',\n",
    "        'Capt': 'Special',\n",
    "        'Sir': 'Special',\n",
    "        'Don': 'Special',\n",
    "        'Lady': 'Special',\n",
    "        'the Countess': 'Miss/Mrs/Ms',\n",
    "        'Jonkheer': 'Special',\n",
    "        'Mlle': 'Miss/Mrs/Ms',\n",
    "        'Ms': 'Miss/Mrs/Ms',\n",
    "        'Mme': 'Miss/Mrs/Ms'\n",
    "    }\n",
    "\n",
    "    X_df['title'] = X_df['name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
    "    \n",
    "    X_df['is_married'] = 0\n",
    "    X_df['is_married'].loc[X_df['title'] == 'Mrs'] = 1\n",
    "\n",
    "    X_df['title'] = X_df['title'].map(title_mapping)\n",
    "    \n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8bc1cc-8712-42b4-9c92-8403041aa845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### create_deck_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c83f24-c959-44c6-809b-0d75a9cbbbaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_deck_feature(X_df):\n",
    "    \"\"\"Creates deck feature\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with new columns\n",
    "    \"\"\"    \n",
    "\n",
    "    X_df['deck'] = X_df['cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71ab017-b10f-446e-82c1-68446aa336bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### one_hot_encode_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6299721-8d49-446a-a79d-8facafe166d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert categorical columns to one-hot encoding features\n",
    "def ohe_categories(X_df):\n",
    "    \"\"\"Creates one-hot encoded (OHE) features for a list of categorical columns \n",
    "    and simplifies column names.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with OHE columns\n",
    "    \"\"\"\n",
    "\n",
    "    # create list of multi-class variables for one-hot encoding\n",
    "    categoricals = ['pclass', 'sex', 'embarked', 'title', 'deck', 'family_count_bin', 'fare_bin','age_bin']\n",
    "\n",
    "    # Without this line, I was just getting another Pclass column. Took about an hour to figure out. This feels really kludgy.\n",
    "    X_df['pclass'] = X_df['pclass'].astype(str)\n",
    "\n",
    "    # create one-hot encoded dummy variables for categoricals\n",
    "    X_df_ohe = pd.get_dummies(X_df[categoricals], drop_first=False, dtype=int)\n",
    "\n",
    "    # leaving this in so that I have an example in the future \n",
    "    X_df_ohe.rename(\n",
    "        columns={'sex_male' : 'sex_male',\n",
    "                 'sex_female' : 'sex_female'\n",
    "                }, inplace = True)\n",
    "    \n",
    "    # concatenate OHE with original df, and drop original category columns\n",
    "    X_df = pd.concat([X_df, X_df_ohe], axis=1)\n",
    "    X_df.drop(categoricals, axis=1, inplace=True)\n",
    "    \n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cedfd20-6922-4a2b-a1e3-313c6c557e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### label_encode_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad71eb18-03b8-4b57-b4d6-3242253b7526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert categorical columns to label encoding features\n",
    "def label_encode_categories(X_df):\n",
    "    \"\"\"Creates label encoded features for a list of categorical columns \n",
    "    and simplifies column names.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame)): same dataframe with label encoded columns\n",
    "    \"\"\"\n",
    "\n",
    "    # create list of multi-class variables for label encoding\n",
    "    categoricals = ['sex', 'embarked', 'title', 'deck', 'family_count_bin', 'fare_bin', 'age_bin']\n",
    "\n",
    "    # create label encoded variables for categoricals\n",
    "    le = LabelEncoder()\n",
    "    for col in categoricals:\n",
    "        X_df[col] = le.fit_transform(X_df[col].astype(str))\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a8e616-ca00-4c97-8ac4-0a4d4a428be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### lower_case_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5824fd76-2c2b-49fb-b6da-f6c3cdb61593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_columns_lowercase(X_df):\n",
    "    \"\"\"Renames all column names to lowercase in place.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): DataFrame whose columns need to be renamed\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    X_df.columns = [col.lower() for col in X_df.columns]\n",
    "\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76610694-5d60-4e01-984c-23946bb0553d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop Unneeded Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d05ee0e-0c90-4273-9ed0-4b52ad75ad8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_columns(X_df, columns_to_drop):\n",
    "    \"\"\"Drops specified columns from the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): DataFrame from which columns need to be dropped\n",
    "    columns_to_drop (list): List of column names to drop\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame): DataFrame with specified columns dropped\n",
    "    \"\"\"\n",
    "    X_df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7cae4c-2849-4885-be87-9a93967bf5ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_specified_columns(X_df):\n",
    "    \"\"\"Drops specified columns from the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame): DataFrame from which columns need to be dropped\n",
    "\n",
    "    Returns:\n",
    "    X_df (pd.DataFrame): DataFrame with specified columns dropped\n",
    "    \"\"\"\n",
    "    columns_to_drop = ['passengerid', 'name', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'family_count']\n",
    "    return drop_columns(X_df, columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e16b3e7-9f9b-400d-b144-a1422a223f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Rename Columns with Bin Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a111ec7-f4b7-4903-81a7-7dc4e42756a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_range_column_names(X_df):\n",
    "    \"\"\"\n",
    "    Renames columns that contain numeric range patterns (e.g., '(0.0, 7.854]') \n",
    "    by replacing them with a simplified, numbered version using the base column name.\n",
    "    For example: 'fare_bin_(0.0, 7.854]' → 'fare_bin_1'\n",
    "    \"\"\"\n",
    "    new_columns = {}\n",
    "    range_pattern = re.compile(r'[\\(\\[].*?[\\)\\]]')  # matches ranges like (x, y] or [x, y)\n",
    "\n",
    "    # Group columns by prefix before the range\n",
    "    prefix_groups = {}\n",
    "\n",
    "    for col in X_df.columns:\n",
    "        match = range_pattern.search(col)\n",
    "        if match:\n",
    "            prefix = col[:match.start()].rstrip('_')\n",
    "            prefix_groups.setdefault(prefix, []).append(col)\n",
    "\n",
    "    # For each group, assign sequential numbers\n",
    "    for prefix, cols in prefix_groups.items():\n",
    "        for i, old_col in enumerate(sorted(cols), start=1):\n",
    "            new_col = f\"{prefix}_{i}\"\n",
    "            new_columns[old_col] = new_col\n",
    "\n",
    "    # Rename in the dataframe\n",
    "    X_df = X_df.rename(columns=new_columns)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89e56cd-7c04-4fd9-8691-13e67fbc6f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Prep Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c225b333-556d-4108-a4f4-2dd2dc50f8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function holds data preparation pipeline for X predictors dataframe\n",
    "def data_prep_pipe(X_df):\n",
    "    \"\"\"Executes data preparation pipeline of steps to clean and transform\n",
    "    an X features dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    X_df (pd.DataFrame)): train or test slice contains predictors\n",
    "\n",
    "    Returns:\n",
    "    X_df_tr (pd.DataFrame)): train or test dataframe, transformed\n",
    "    \"\"\"\n",
    "    \n",
    "    # instantiate custom transformer functions\n",
    "    get_fill_age = FunctionTransformer(fill_age, validate=False)\n",
    "    get_fill_embarked = FunctionTransformer(fill_embarked, validate=False)\n",
    "    get_fill_fare = FunctionTransformer(fill_fare, validate=False)\n",
    "    get_bin_age = FunctionTransformer(bin_age, validate=False)\n",
    "    get_bin_fare = FunctionTransformer(bin_fare, validate=False)\n",
    "    get_bin_family_count = FunctionTransformer(bin_family_count, validate=False)\n",
    "    get_create_title_feature = FunctionTransformer(create_title_feature, validate=False)\n",
    "    get_create_deck_feature = FunctionTransformer(create_deck_feature, validate=False)\n",
    "    get_ohe_categories = FunctionTransformer(ohe_categories, validate=False)\n",
    "    get_label_encode_categeories = FunctionTransformer(label_encode_categories, validate=False)\n",
    "    get_rename_columns_lowercase = FunctionTransformer(rename_columns_lowercase, validate=False)\n",
    "    get_drop_specified_columns = FunctionTransformer(drop_specified_columns, validate=False)\n",
    "    get_clean_range_column_names = FunctionTransformer(clean_range_column_names, validate=False)\n",
    "\n",
    "    # instantiate data prep pipeline object and steps\n",
    "    prep_pipe = Pipeline(memory=None, \n",
    "                         steps=[('rename_columns_lowercase', get_rename_columns_lowercase),\n",
    "                                ('fill_age', get_fill_age),\n",
    "                                ('fill_embarked', get_fill_embarked),\n",
    "                                ('fill_fare', get_fill_fare),\n",
    "                                ('bin_age', get_bin_age),\n",
    "                                ('bin_fare', get_bin_fare),\n",
    "                                ('bin_family_count', get_bin_family_count),\n",
    "                                ('create_title_feature', get_create_title_feature),\n",
    "                                ('create_deck_feature', get_create_deck_feature),\n",
    "                                # Switched to label encoding for everything - imrpoved accuracy by 1%\n",
    "                                #('ohe_categories', get_ohe_categories),\n",
    "                                ('label_encode_categories', get_label_encode_categeories),\n",
    "                                ('drop_specified_columns', get_drop_specified_columns),\n",
    "                                ('rename_columns_lowercase_again', get_rename_columns_lowercase),\n",
    "                                ('clean_range_column_names', get_clean_range_column_names)\n",
    "                                ])\n",
    "    \n",
    "    # apply data prep pipeline to df and store/return new df\n",
    "    X_df_tr = prep_pipe.fit_transform(X_df)\n",
    "    return X_df_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4a4b45-52fd-47fe-a54d-ab564016c299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894534ea-bca3-41d4-b901-cce86a93bec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d87f79-461a-4193-82aa-58ca4e60e43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create X predictors and y target variable\n",
    "y = df['Survived']\n",
    "X = df.drop(columns=['Survived'], axis=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "SEED = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af07ca8f-1e9e-4bd7-ac4b-78c92b725b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Calculate Persisted Data Prep Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182d2351-bed0-4487-97f1-a6535a5993ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate and persist the median ages and fares to fill in the missing data. This is done against the training set only to prevent leakage.\n",
    "# Eventually I would refactor this to use the pipeline - just don't have time right now!\n",
    "# Note: Evitan used the union of training and test data to calculate averages. I believe this may be an error. https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial block In [7]\n",
    "\n",
    "process_and_persist_median_ages(X_train)\n",
    "process_and_persist_fare(X_train)\n",
    "calculate_and_persist_fare_bins(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbfa2fd3-6a98-418d-8ff0-8d811f8b0c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2783b784-e862-4cec-abbc-1dd158cc9bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# send both X_train and X_test through data prep steps\n",
    "X_train = data_prep_pipe(X_train)\n",
    "X_test = data_prep_pipe(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9251e4af-f446-496d-9a72-d762102aea40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Align Train and Test Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94141c03-2ece-4321-be43-91413cdf5d41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be1ee5b-5666-4485-8cb7-28083548353b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "169cf852-ca51-4cc5-9947-d8642690eccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_test.info(\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3715a6a-8d42-49cb-b36d-c0d0cb89ce85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08e6b854-9b9a-4c94-9219-516ab88624d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run Models with Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1151c9-c9cd-4108-84f9-8de6e0ee2380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Define a models list with parameter stubs\n",
    "models_default = DataFrame({\n",
    "    'model_name': ['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'GradientBoostingClassifier', 'XGBClassifier', 'LGBMClassifier'],\n",
    "    'model_instance': [\n",
    "        LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100),\n",
    "        DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None),\n",
    "        RandomForestClassifier(max_depth=10, min_samples_leaf=2),\n",
    "        #RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=10, min_samples_leaf=2),\n",
    "        #RandomForestClassifier(criterion='gini', bootstrap=True, max_depth=10, max_features=None,min_samples_leaf=2, min_samples_split=2, n_estimators=100),\n",
    "\n",
    "        #RandomForestClassifier(criterion='gini',bootstrap=True, max_depth=5, max_features=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200),\n",
    "        GradientBoostingClassifier(loss='log_loss', learning_rate=0.1, n_estimators=100),\n",
    "        XGBClassifier(objective='binary:logistic', learning_rate=0.1, n_estimators=100),\n",
    "        LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, n_estimators=100)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Train the models using the training features and labels\n",
    "for index, row in models_default.iterrows():\n",
    "    model = row['model_instance']\n",
    "    model.fit(X_train, y_train)\n",
    "    # Report trained model\n",
    "    print(f'Trained and ready: {row[\"model_name\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8749ee1-800c-4851-b2b4-779d728bffe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test all models by running models list through a for loop\n",
    "\n",
    "accuracy_scores = []\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for index, row in models_default.iterrows():\n",
    "    model = row['model_instance']\n",
    "    # Use the model to generate predictions for the Test split, based on its features only\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # Compare model's predictive performance to the provided test labels\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred_proba) * 100 if y_pred_proba is not None else None\n",
    "    auc_scores.append(auc)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred) * 100\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Report the model and its scores\n",
    "    \"\"\"print(row['model_name'])\n",
    "    print(f'  Accuracy: {accuracy}')\n",
    "    print(f'  AUC: {auc}')\n",
    "    print(f'  F1: {f1}\\n')\"\"\"\n",
    "\n",
    "# Add the accuracy, AUC, and F1 scores to the models dataframe\n",
    "models_default['accuracy_score'] = accuracy_scores\n",
    "models_default['auc_score'] = auc_scores\n",
    "models_default['f1_score'] = f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "482a7d31-30ab-4a3f-b29f-7f940cd3d973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create ROC Curve and Confusion Matrix for each model\n",
    "\n",
    "for index, row in models_default.iterrows():\n",
    "    model = row['model_instance']\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # ROC Curve\n",
    "    if y_pred_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'{row[\"model_name\"]} (AUC = {row[\"auc_score\"]:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve for {row[\"model_name\"]}')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for {row[\"model_name\"]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa324e1-6c46-46a1-9c63-40e0210e0cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730438de-7aa8-428f-b18d-586f544f547e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "88c4b2e6-03d4-4333-8507-465aa445fddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate average survival grouped by distinct values in each column of X_train\n",
    "average_survival_by_column = {}\n",
    "\n",
    "for column in X_train.columns:\n",
    "    avg_survival = X_train.join(y_train).groupby(column)[y_train.name].mean()\n",
    "    average_survival_by_column[column] = avg_survival\n",
    "\n",
    "# Display the results\n",
    "for column, avg_survival in average_survival_by_column.items():\n",
    "    print(f\"Average survival grouped by {column}:\")\n",
    "    display(avg_survival)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0562aac-ae9e-49e6-b67d-40d6f5072110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88b0f7d-7d82-42f0-8c25-4dcab40b7153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grid Search for Hyperparameter Tuning\n",
    "# Define your models\n",
    "models = DataFrame({\n",
    "    'model_name': [#'LogisticRegression',\n",
    "                   #'DecisionTreeClassifier', \n",
    "                   'RandomForestClassifier'#, \n",
    "                   #'GradientBoostingClassifier', \n",
    "                   #'XGBClassifier', \n",
    "                   #'LGBMClassifier'\n",
    "                   ],\n",
    "    'model_instance': [#LogisticRegression(),\n",
    "                       #DecisionTreeClassifier(),\n",
    "                       RandomForestClassifier()#, \n",
    "                       #GradientBoostingClassifier(),\n",
    "                       #XGBClassifier(),\n",
    "                       #LGBMClassifier()\n",
    "                       ]\n",
    "})\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'solver': ['lbfgs']\n",
    "    },\n",
    "    'DecisionTreeClassifier': {\n",
    "        'max_depth': [3, 5, 10, None],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [100, 200, 500, 1000],  # More trees → better performance, longer training\n",
    "        'max_depth': [5, 10, 20],             # Controls how deep trees grow\n",
    "        'min_samples_leaf': [1, 2, 4],              # Prevents tiny splits (reduces overfitting)\n",
    "        'min_samples_split': [2, 5, 10],            # Controls when to split a node\n",
    "        'max_features': ['sqrt', 'log2', None],     # How many features to consider per split\n",
    "        'bootstrap': [True, False]                  # Whether to sample data with replacement\n",
    "    },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5]\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'LGBMClassifier': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'num_leaves': [15, 31],\n",
    "        'max_depth': [-1, 5]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop through each model and just fit the best one\n",
    "fitted_models = {}\n",
    "\n",
    "for index, row in models.iterrows():\n",
    "    name = row['model_name']\n",
    "    base_model = row['model_instance']\n",
    "    print(f\"\\n🔍 Tuning and fitting {name}...\")\n",
    "\n",
    "    param_grid = param_grids.get(name, {})\n",
    "\n",
    "    search = GridSearchCV(base_model, param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = search.best_estimator_\n",
    "    fitted_models[name] = best_model  # Store it for later use\n",
    "\n",
    "    print(f'✅ Best Parameters for {name}: {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d3dd1963-e2ea-4a45-800e-53eed4f96a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "# Prepare lists to hold scores\n",
    "accuracies = []\n",
    "aucs = []\n",
    "f1s = []\n",
    "\n",
    "# Loop through models in the DataFrame\n",
    "for index, row in models.iterrows():\n",
    "    name = row['model_name']\n",
    "    model = fitted_models.get(name)\n",
    "\n",
    "    if model:\n",
    "        # Get predicted labels and probabilities\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = model.predict(X_test)  # uses 0.5 threshold\n",
    "\n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test, y_pred) * 100\n",
    "        auc = roc_auc_score(y_test, y_proba) * 100\n",
    "        f1 = f1_score(y_test, y_pred) * 100\n",
    "\n",
    "        # Append to results\n",
    "        accuracies.append(acc)\n",
    "        aucs.append(auc)\n",
    "        f1s.append(f1)\n",
    "    else:\n",
    "        accuracies.append(None)\n",
    "        aucs.append(None)\n",
    "        f1s.append(None)\n",
    "\n",
    "# Add metrics to the models DataFrame\n",
    "models['Accuracy'] = accuracies\n",
    "models['AUC'] = aucs\n",
    "models['F1'] = f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "284b3678-3b2e-41ac-8fe7-b01d09ab14ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final_Project_Titanic_Dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
